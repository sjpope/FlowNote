{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5b0ad54-b137-46de-b06b-db6664560eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, how are you? I'm so glad you're here. I'm so glad you're here. I'm so glad you're here. I'm so glad you're here. I'm so glad you're here. I'm so glad\n"
     ]
    }
   ],
   "source": [
    "#import sys\n",
    "#sys.path.append(\"C:/Users/sampo/OneDrive/Documents/TXST/SWE/Repos/flownote\")\n",
    "import os\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel #, GPTNeoForCausalLM\n",
    "import logging\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import torch \n",
    "\n",
    "# 355M Params\n",
    "#model_path = r'C:/Users/sampo/OneDrive/Documents/TXST/SWE/Repos/flownote/models'\n",
    "#tokenizer_path = r'C:/Users/sampo/OneDrive/Documents/TXST/SWE/Repos/flownote/tokenizer'\n",
    "#tokenizer = GPT2Tokenizer.from_pretrained(tokenizer_path)\n",
    "#model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "\n",
    "# 774M Params\n",
    "# hf_token = os.getenv('HF_TOKEN')\n",
    "# model_name = \"gpt2-large\" \n",
    "save_directory = r'D:/gpt2-large'\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(save_directory)\n",
    "model = GPT2LMHeadModel.from_pretrained(save_directory)\n",
    "\n",
    "\n",
    "\n",
    "# Example of using the model (optional)\n",
    "# You might want to add an example of how to use the tokenizer and model to generate text.\n",
    "# For example:\n",
    "# inputs = tokenizer(\"Hello, how are you?\", return_tensors=\"pt\")\n",
    "# outputs = model.generate(**inputs, max_length=50)\n",
    "# print(tokenizer.decode(outputs[0]))\n",
    "\n",
    "\n",
    "\n",
    "#from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "#model_name = \"EleutherAI/gpt-neo-2.7B\"  # You can choose a different model size based on your requirement\n",
    "#tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=hf_token)\n",
    "#model = AutoModelForCausalLM.from_pretrained(model_name, use_auth_token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d095eeab-c04e-4821-919d-4e7743d54696",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_path = r'C:/Users/sampo/OneDrive/Documents/TXST/SWE/Repos/flownote/models'\n",
    "#tokenizer_path = r'C:/Users/sampo/OneDrive/Documents/TXST/SWE/Repos/flownote/tokenizer'\n",
    "save_directory = r'D:/gpt2-large'\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "model.save_pretrained(save_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4637d4f-228b-4efd-95f6-6efec9151a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_content_vocab_Neo(prompt, max_length=50, num_return_sequences=3):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=max_length, truncation=True).input_ids\n",
    "    max_new_tokens=50\n",
    "    outputs = model.generate(\n",
    "        inputs, \n",
    "        max_new_tokens=max_new_tokens,\n",
    "        no_repeat_ngram_size=2,\n",
    "         \n",
    "        num_return_sequences=num_return_sequences, \n",
    "\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    keywords = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e23ae464-aefc-4532-81e2-357b04d13cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process functions adapted from AIEngine\n",
    "\n",
    "def strip_prompt(prompt, content):\n",
    "    try:\n",
    "        start_index = content.index(prompt) + len(prompt)\n",
    "    except ValueError:\n",
    "        start_index = 0\n",
    "    return content[start_index:].strip()\n",
    "    \n",
    "\n",
    "def generate_content(prompt, max_length=150, num_return_sequences=1, additional_tokens=500):\n",
    "    encoding = tokenizer(prompt, return_tensors='pt', truncation=True)\n",
    "    input_ids = encoding['input_ids']\n",
    "    attention_mask = encoding['attention_mask']\n",
    "\n",
    "    total_max_length = input_ids.shape[1] + additional_tokens\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=total_max_length,\n",
    "            min_length=input_ids.shape[1] + 20,\n",
    "            no_repeat_ngram_size=3,\n",
    "            num_return_sequences=num_return_sequences,\n",
    "            temperature=0.8,\n",
    "            top_k=50,\n",
    "            top_p=0.92,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    suggestions = [tokenizer.decode(gid, skip_special_tokens=True) for gid in generated_ids]\n",
    "    return suggestions\n",
    "\n",
    "def generate_content_vocab(prompt, max_length=50, num_return_sequences=3, additional_tokens=50):\n",
    "    encoding = tokenizer(prompt, return_tensors='pt', truncation=True)\n",
    "    input_ids = encoding['input_ids']\n",
    "    attention_mask = encoding['attention_mask']\n",
    "    \n",
    "    if input_ids.shape[1] > 10:\n",
    "        min_length = input_ids.shape[1]\n",
    "    else:\n",
    "        min_length = 10\n",
    "    \n",
    "    total_max_length = input_ids.shape[1] + additional_tokens\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=total_max_length,\n",
    "            min_length=input_ids.shape[1] + 20,\n",
    "            no_repeat_ngram_size=3,\n",
    "            num_return_sequences=num_return_sequences,\n",
    "            temperature=0.5,\n",
    "            top_k=20,\n",
    "            top_p=0.75,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "        # suggested .3, 10, .6\n",
    "\n",
    "    # .replace('.', '').replace('and', ',')       for gid in gen...\n",
    "    suggestions = [tokenizer.decode(gid, skip_special_tokens=True) for gid in generated_ids]\n",
    "    return suggestions\n",
    "\n",
    "def summarize(content):\n",
    "    summary_prompt = f\"Summarize this content: {content}\"\n",
    "    summary = generate_content(summary_prompt, num_return_sequences=1)[0]\n",
    "    res = strip_prompt(summary_prompt, summary)\n",
    "    return res\n",
    "\n",
    "def generate_keywords(content):\n",
    "    # prompt = f\"Identify the key concepts in this text: {content}\"\n",
    "    # prompt = f\"Extract the most important keywords that represent the central themes of this text: {content}\"\n",
    "    # prompt = f\"Return only a comma separated list of the most important keywords relevant to this text: {content}\"\n",
    "    prompt = f\"Please list concise, individual keywords (separated by commas) representing the core themes of this text: {content}\"\n",
    "\n",
    "    contents = generate_content_vocab(prompt, num_return_sequences=3)\n",
    "    \n",
    "    #res = []\n",
    "    #for content in contents:\n",
    "        #res.append(strip_prompt(prompt, content))\n",
    "        \n",
    "    res = [strip_prompt(prompt, content) for content in contents]\n",
    "    \n",
    "    return res\n",
    "\n",
    "def autocomplete(content, additional_tokens=50):\n",
    "    completion = generate_content(content, additional_tokens=additional_tokens, num_return_sequences=1)[0]\n",
    "    return completion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4faf3eeb-f51d-4683-9f7f-6cd21851b926",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_summarization(contents):\n",
    "    \"\"\"Test summarization on specified contents.\"\"\"\n",
    "    for idx, content in enumerate(contents, start=1):\n",
    "        print(f\"\\n--- Content {idx} Summary ---\")\n",
    "        print(\"Original Content:\")\n",
    "        print(content)\n",
    "        print(\"\\nSummary:\")\n",
    "        print(summarize(content))\n",
    "\n",
    "def test_vocab_extraction(contents):\n",
    "    \"\"\"Test vocabulary extraction on specified contents.\"\"\"\n",
    "    for idx, content in enumerate(contents, start=1):\n",
    "        print(f\"\\n--- Content {idx} Vocabulary Extraction ---\")\n",
    "        print(\"Original Content:\")\n",
    "        print(content)\n",
    "        print(\"\\nExtracted Keywords:\")\n",
    "        \n",
    "        contents = generate_keywords(content)\n",
    "        [print(f'{note}\\n') for note in contents]\n",
    "        \n",
    "\n",
    "def test_autocompletion(contents):\n",
    "    \"\"\"Test autocompletion on specified contents.\"\"\"\n",
    "    for idx, content in enumerate(contents, start=1):\n",
    "        print(f\"\\n--- Content {idx} Autocompletion ---\")\n",
    "        print(\"Original Content:\")\n",
    "        print(content)\n",
    "        print(\"\\nAutocompleted Text:\")\n",
    "        print(autocomplete(content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "411e06a6-2e6d-45ec-b29d-8e3c227c6618",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Vocabulary Extraction\n",
      "\n",
      "--- Content 1 Vocabulary Extraction ---\n",
      "Original Content:\n",
      "Artificial intelligence in healthcare has transformed the way patient data is analyzed and interpreted. By leveraging machine learning algorithms, clinicians can now predict patient outcomes with greater accuracy and personalize treatment plans effectively. This paradigm shift towards data-driven medicine necessitates a comprehensive understanding of ethical considerations, such as patient privacy concerns and the potential for bias in algorithmic decisions.\n",
      "\n",
      "Extracted Keywords:\n",
      "This paper provides a comprehensive overview of the ethical issues surrounding the use of machine learning in healthcare. The paper concludes with a discussion of the implications of the emerging field of machine intelligence for healthcare.\n",
      "\n",
      "Abstract\n",
      "\n",
      "This paper reviews the ethical and legal\n",
      "\n",
      "This paper explores the ethical implications of the use of machine learning in healthcare, and the ethical challenges of using such technology in the context of medical ethics.\n",
      "\n",
      "Introduction\n",
      "\n",
      "In the past decade, the use and interpretation of patient data has become increasingly\n",
      "\n",
      "This paper presents a framework for understanding the ethical implications of machine learning in healthcare, and the ethical challenges that arise from the use of such algorithms. It discusses the ethical issues associated with the use and abuse of machine-learning algorithms in healthcare and outlines the\n",
      "\n",
      "\n",
      "--- Content 2 Vocabulary Extraction ---\n",
      "Original Content:\n",
      "The impact of climate change on marine biodiversity has become increasingly apparent, with rising sea temperatures causing widespread coral bleaching events. These ecological disturbances threaten the intricate food webs supported by coral reefs and may lead to substantial losses in marine species diversity. Researchers are urgently investigating adaptive strategies that marine organisms might employ to survive in these rapidly changing environments.\n",
      "\n",
      "Extracted Keywords:\n",
      "The impact of sea level rise on coral reefs has become more apparent, as sea levels continue to rise. Coral reefs are among the most diverse ecosystems on Earth, and their survival is critical to the health of the marine ecosystem. Coral bleaching\n",
      "\n",
      "The impact of global warming on marine species has become more apparent, and this is likely to have a major impact on marine ecosystems. In particular, the impact of rising sea temperature on coral reefs has become apparent, as coral bleached in the\n",
      "\n",
      "The impact of global warming on marine species has become more apparent, as rising sea temperature causes widespread coral reef bleaching. These disturbances threaten to reduce the food webs that support coral reefs, and may ultimately lead to significant losses in the species diversity\n",
      "\n",
      "\n",
      "--- Content 3 Vocabulary Extraction ---\n",
      "Original Content:\n",
      "Neurolinguistics explores the brain mechanisms underlying language comprehension and production, offering insights into the cognitive processes involved in linguistic tasks. Recent studies using functional magnetic resonance imaging (fMRI) have identified specific brain regions associated with syntactic processing and semantic analysis, contributing to a better understanding of language disorders such as dyslexia and aphasia.\n",
      "\n",
      "Extracted Keywords:\n",
      "Neurolingual approaches to language processing have also been applied to the study of neuropsychological deficits in autism, schizophrenia, and other neurological disorders.\n",
      "\n",
      "Neurolinguistic approaches to the cognitive processing of language have been applied in a variety of\n",
      "\n",
      "This text also explores the neural mechanisms underlying the acquisition of language, including the role of the prefrontal cortex, the anterior cingulate cortex, and the temporal lobe. Neurolingua explores the neurobiology of language and the neural basis of language comprehension,\n",
      "\n",
      "The purpose of this book is to provide a comprehensive overview of the neuroscientific research on language comprehension, with a focus on the cognitive mechanisms underlying the production of language. It will also provide an overview of research on the neural basis of language production, with\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "autocomplete_notes = [\n",
    "    \"Despite the rapid advancements in renewable energy technology, the integration of these systems into the national grid...\",\n",
    "    \"The sociopolitical implications of biotechnology in the 21st century have led to significant debates concerning...\",\n",
    "    \"In the context of quantum computing, the coherence time of qubits is essential for...\"\n",
    "]\n",
    "\n",
    "vocab_notes = [\n",
    "    \"Artificial intelligence in healthcare has transformed the way patient data is analyzed and interpreted. By leveraging machine learning algorithms, clinicians can now predict patient outcomes with greater accuracy and personalize treatment plans effectively. This paradigm shift towards data-driven medicine necessitates a comprehensive understanding of ethical considerations, such as patient privacy concerns and the potential for bias in algorithmic decisions.\",\n",
    "    \"The impact of climate change on marine biodiversity has become increasingly apparent, with rising sea temperatures causing widespread coral bleaching events. These ecological disturbances threaten the intricate food webs supported by coral reefs and may lead to substantial losses in marine species diversity. Researchers are urgently investigating adaptive strategies that marine organisms might employ to survive in these rapidly changing environments.\",\n",
    "    \"Neurolinguistics explores the brain mechanisms underlying language comprehension and production, offering insights into the cognitive processes involved in linguistic tasks. Recent studies using functional magnetic resonance imaging (fMRI) have identified specific brain regions associated with syntactic processing and semantic analysis, contributing to a better understanding of language disorders such as dyslexia and aphasia.\"\n",
    "]\n",
    "\n",
    "\n",
    "summarize_notes = [\n",
    "    \"The integration of Internet of Things (IoT) devices into urban infrastructure represents a significant step towards smart city development. IoT technology allows for the real-time monitoring and management of city resources, which leads to more efficient use of energy, improved traffic management, and enhanced public safety. The data collected by these devices can be used to analyze patterns of resource use, predict maintenance issues, and coordinate emergency responses more effectively. However, the widespread adoption of IoT solutions also raises concerns about data security and the privacy of citizens.\",\n",
    "    \"Economic globalization has led to unprecedented interconnectedness between markets around the world, facilitating the rapid spread of capital and goods but also creating systemic risks. Financial crises in one country can now have ripple effects globally, as seen in the 2008 financial meltdown. Policymakers are tasked with devising regulations that safeguard economic stability without stifling innovation. This balance is crucial in maintaining a resilient global economy that can withstand shocks and support sustainable growth.\",\n",
    "    \"The role of artificial intelligence in enhancing educational outcomes cannot be overstated. AI-driven platforms can adapt to individual learning styles and provide personalized feedback, potentially revolutionizing the educational landscape. These technologies also enable the scaling of quality education to underserved populations, breaking down barriers to access. However, the deployment of AI in education must be carefully managed to avoid reinforcing existing disparities in educational quality and access.\"\n",
    "]\n",
    "\n",
    "\n",
    "#print(\"Testing Summarization\")\n",
    "#test_summarization(summarize_notes)\n",
    "print(\"\\nTesting Vocabulary Extraction\")\n",
    "test_vocab_extraction(vocab_notes)\n",
    "#print(\"\\nTesting Autocompletion\")\n",
    "#test_autocompletion(autocomplete_notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22f9982-66d8-4911-96ea-bcf5613c14c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
